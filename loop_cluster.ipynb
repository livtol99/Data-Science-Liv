{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# General packages\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from time import time\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Data processing and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "# Machine learning and forecasting\n",
    "import darts\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts import TimeSeries\n",
    "from darts.models import TransformerModel, ExponentialSmoothing\n",
    "from darts.metrics import mape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "\n",
    "# Time series forecasting with Prophet\n",
    "import prophet\n",
    "from prophet import Prophet\n",
    "\n",
    "# PySpark for distributed processing\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Deep learning with Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Machine learning and clustering\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "from tslearn.clustering import silhouette_score, TimeSeriesKMeans\n",
    "from tslearn.datasets import CachedDatasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "# Custom functions\n",
    "%run functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping the cluster code over all cluster data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:28:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:28:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:28:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the folder\n",
    "data_folder_path = '/work/Data-Science-Liv/clustered_data'\n",
    "forecast_folder_path = '/work/Data-Science-Liv/cluster_forecasts'\n",
    "\n",
    "# Create the folders if they don't exist\n",
    "if not os.path.exists(data_folder_path):\n",
    "    os.makedirs(data_folder_path)\n",
    "\n",
    "if not os.path.exists(forecast_folder_path):\n",
    "    os.makedirs(forecast_folder_path)\n",
    "\n",
    "# Define the file pattern to match\n",
    "file_pattern = 'long_cluster_*_full.csv'\n",
    "\n",
    "# Get a list of file paths that match the pattern\n",
    "data_file_paths = glob.glob(f\"{data_folder_path}/{file_pattern}\")\n",
    "\n",
    "# Initialize empty dictionaries to store forecasts for each cluster\n",
    "forecasts_clusters = {}\n",
    "\n",
    "# Iterate over the file paths\n",
    "for data_file_path in data_file_paths:\n",
    "    \n",
    "    # Extract the cluster number from the file name\n",
    "    match = re.search(r'long_cluster_(\\d+)_full.csv', data_file_path)\n",
    "    if match:\n",
    "        cluster = int(match.group(1))\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(data_file_path, parse_dates=['Year'])\n",
    "\n",
    "    # Create test and train splits\n",
    "    train_df, test_df = create_splits(df)\n",
    "\n",
    "    # Obtain the representative train ts via yearly aggregation across all individual time series in the current cluster\n",
    "    representative_df_train = create_representative_train(train_df)\n",
    "\n",
    "    # Define model to be fit to the representative\n",
    "    model = Prophet(\n",
    "        growth='linear',\n",
    "        seasonality_mode='additive',\n",
    "        yearly_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False\n",
    "    )\n",
    "    \n",
    "    # Fit model to representative train ts\n",
    "    model.fit(representative_df_train)\n",
    "\n",
    "    #  Obtain the representative test ts\n",
    "    representative_df_test = create_representative_test(test_df)\n",
    "    \n",
    "    #Generate forecasts on the test period for the representative test ts\n",
    "    representative_forecasts = model.predict(df=representative_df_test.drop('y', axis=1))\n",
    "\n",
    "    # Create the adjustment series and combine together train, test and all adjustment series\n",
    "    df_actual_all = create_adjustment_series(train_df, test_df, representative_df_train, representative_df_test, representative_forecasts)\n",
    "\n",
    "    ####  Generate forecasts for each countries time series \n",
    "\n",
    "    #Create adjustment forecasts  by fitting a simple linear model to the adjustment series\n",
    "    adjustment_forecasts_df = df_actual_all.groupby(['Country']).apply(lambda x: get_linear_model_pred(x, representative_df_test)).reset_index(drop=False)\n",
    "    adjustment_forecasts_df = adjustment_forecasts_df.rename(columns={'Year': 'ds'})\n",
    "\n",
    "    #Merge all data so far with the adjustment forecasts\n",
    "    final_output = pd.merge(df_actual_all, adjustment_forecasts_df, on=['Country', 'ds'], how='left')\n",
    "    \n",
    "    #Create forecasts for all individual time series in a cluster by adding together the representative forecasys and adjustment forecasts\n",
    "    final_output['final_prediction'] = final_output['yhat'] + final_output['adjustment_Forecasts_test']\n",
    "\n",
    "    # Store forecasts in a diactionary based on the current cluster\n",
    "    forecasts_clusters[cluster] = final_output[['Country', 'ds', 'final_prediction']]\n",
    "\n",
    "    # Save the current forecast DataFrame as a CSV file\n",
    "    file_name = f'forecasts_cluster{cluster}.csv'\n",
    "    file_path = os.path.join(forecast_folder_path, file_name)\n",
    "    forecasts_clusters[cluster].to_csv(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
