{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Cluster Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install and import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "# multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "#hmm needed? \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch import nn\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#darts\n",
    "# transformers and preprocessing\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.utils import SeasonalityMode, TrendMode, ModelMode\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.models import * #everything\n",
    "\n",
    "#loss metrics\n",
    "from darts.metrics import mape\n",
    "from darts.metrics import smape\n",
    "from darts.utils.losses import SmapeLoss\n",
    "\n",
    "# likelihood\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "# settings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "#hyperparameter tuning\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_contour,\n",
    "    plot_param_importances,\n",
    ")\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "#extras\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "Consider making this a bit more smooth (glob.glob to access all files at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1 = pd.read_csv('/work/Data-Science-Liv/cluster_forecasts/forecasts_cluster0.csv', index_col=0)\n",
    "cl2 = pd.read_csv('/work/Data-Science-Liv/cluster_forecasts/forecasts_cluster1.csv', index_col=0)\n",
    "cl3 = pd.read_csv('/work/Data-Science-Liv/cluster_forecasts/forecasts_cluster2.csv', index_col=0)\n",
    "cl4 = pd.read_csv('/work/Data-Science-Liv/cluster_forecasts/forecasts_cluster3.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data inspection\n",
    "- Removing NA rows from years included in the training set\n",
    "- Making sure the year span is correct\n",
    "- Checking the unique countries in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with NAs - these rows represent the first years in the data that were used for training. Here we only have the test forecasts\n",
    "cl1 = cl1.dropna(subset=['final_prediction'])\n",
    "cl2 = cl2.dropna(subset=['final_prediction'])\n",
    "cl3 = cl3.dropna(subset=['final_prediction'])\n",
    "cl4 = cl4.dropna(subset=['final_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are there any NAs in the dfs now?\n",
    "data_frames = [cl1, cl2, cl3, cl4]\n",
    "\n",
    "# Check if any of the data frames contain NAs\n",
    "for df in data_frames:\n",
    "    if df.isna().any().any():\n",
    "        print(f\"At least one NA value found in DataFrame {df}.\")\n",
    "        break\n",
    "else:\n",
    "    print(\"No NA values found in any of the data frames.\")\n",
    "\n",
    "# There are no NAs in the dfs now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there overlapping countries in the data frames?\n",
    "\n",
    "data_frames = [cl1, cl2, cl3, cl4]\n",
    "common_countries = set()\n",
    "\n",
    "# Compare unique countries pairwise between data frames\n",
    "for i in range(len(data_frames)):\n",
    "    for j in range(i+1, len(data_frames)):\n",
    "        countries_i = set(data_frames[i]['Country'].unique())\n",
    "        countries_j = set(data_frames[j]['Country'].unique())\n",
    "        \n",
    "        if countries_i & countries_j:  # Check if there are common countries\n",
    "            common_countries.update(countries_i & countries_j)\n",
    "\n",
    "# Check if there are any common countries among the data frames\n",
    "if common_countries:\n",
    "    print(\"At least two data frames have common countries.\")\n",
    "else:\n",
    "    print(\"No common countries found among the data frames.\")\n",
    "\n",
    "\n",
    "#The clusters do not contain the same countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the years over which the data span\n",
    "year_unique1 = set(cl1['ds'].unique())\n",
    "year_unique2 = set(cl2['ds'].unique())\n",
    "year_unique3 = set(cl3['ds'].unique())\n",
    "year_unique4 = set(cl4['ds'].unique())\n",
    "\n",
    "#All dfs has the right testing range (span from 2004-2001)\n",
    "\n",
    "#Inspecting the entire data frame\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(cl4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where the validation goes down\n",
    "- sMAPE\n",
    "\n",
    "Symmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors.\n",
    "It is a metric used to evaluate the accuracy of a forecasting model or compare the performance of different forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eva's code\n",
    "# Code for calculating sMAPE\n",
    "# countries er en liste af lande som jeg trÃ¦kker ud fra pandas dataframe inden dataet bliver lavet om til timeseries, countries = list(data_countries[\"Country\"].unique())\n",
    "def eval_local_model(\n",
    "    ID_list: List[str],\n",
    "    train_series: List[TimeSeries], \n",
    "    test_series: List[TimeSeries], \n",
    "    model_cls, \n",
    "    **kwargs\n",
    ") -> Tuple[List[float], float, List[Tuple[str, float]]]:\n",
    "\n",
    "    ''' Fitting the model of choice to training data. Retrieving predictions for a given forecasting horizon.\n",
    "    Extracting computation time. Extracting sMAPEs per time-series and the time-series with highest sMAPEs. '''\n",
    "\n",
    "    #define empty lists for predictions and processing time\n",
    "    preds = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Fit model and predict for each series individually\n",
    "    print(\"fitting models...\")\n",
    "    for series in tqdm(train_series):\n",
    "        model = model_cls(**kwargs)\n",
    "        model.fit(series)\n",
    "        pred = model.predict(n=HORIZON)\n",
    "        preds.append(pred)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    name = model.__class__.__name__\n",
    "\n",
    "    #Apply eval_forecasts function to extract sMAPEs and plot\n",
    "    print(\"extracting sMAPEs...\")\n",
    "    smapes = eval_forecasts(name, preds, test_series)\n",
    "\n",
    "    #Extract sMAPES per time-series and time-series with highest sMAPEs\n",
    "    smapes_pr_country = list(zip(countries, smapes))\n",
    "    highest_smapes = heapq.nlargest(10, smapes_pr_country, key=lambda x: x[1])\n",
    "    \n",
    "    return smapes, elapsed_time, smapes_pr_country, highest_smapes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
