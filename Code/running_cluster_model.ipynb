{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# General packages\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from time import time\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Data processing and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "# Machine learning and forecasting\n",
    "import darts\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts import TimeSeries\n",
    "from darts.models import TransformerModel, ExponentialSmoothing\n",
    "from darts.metrics import mape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "\n",
    "# Time series forecasting with Prophet\n",
    "import prophet\n",
    "from prophet import Prophet\n",
    "\n",
    "# PySpark for distributed processing\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Deep learning with Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Machine learning and clustering\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "from tslearn.clustering import silhouette_score, TimeSeriesKMeans\n",
    "from tslearn.datasets import CachedDatasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "# Custom functions\n",
    "%run functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping the cluster code over all cluster data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:51:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the folder\n",
    "data_folder_path = '/work/Data-Science-Liv/clustered_data'\n",
    "forecast_folder_path = '/work/Data-Science-Liv/cluster_forecasts'\n",
    "\n",
    "# Create the folders if they don't exist\n",
    "if not os.path.exists(data_folder_path):\n",
    "    os.makedirs(data_folder_path)\n",
    "\n",
    "if not os.path.exists(forecast_folder_path):\n",
    "    os.makedirs(forecast_folder_path)\n",
    "\n",
    "# Define the file pattern to match\n",
    "file_pattern = 'long_cluster_*_full.csv'\n",
    "\n",
    "# Get a list of file paths that match the pattern\n",
    "data_file_paths = glob.glob(f\"{data_folder_path}/{file_pattern}\")\n",
    "\n",
    "# Initialize empty dictionaries to store forecasts for each cluster\n",
    "forecasts_clusters = {}\n",
    "\n",
    "# Iterate over the file paths\n",
    "for data_file_path in data_file_paths:\n",
    "    \n",
    "    # Extract the cluster number from the file name\n",
    "    match = re.search(r'long_cluster_(\\d+)_full.csv', data_file_path)\n",
    "    if match:\n",
    "        cluster = int(match.group(1))\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(data_file_path, parse_dates=['Year'])\n",
    "\n",
    "    # Create test and train splits\n",
    "    train_df, test_df = create_splits(df)\n",
    "\n",
    "    # Obtain the representative train ts via yearly aggregation across all individual time series in the current cluster\n",
    "    representative_df_train = create_representative_train(train_df)\n",
    "\n",
    "    # Define model to be fit to the representative\n",
    "    model = Prophet(\n",
    "        growth='linear',\n",
    "        seasonality_mode='additive',\n",
    "        yearly_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False)\n",
    "    \n",
    "    # Fit model to representative train ts\n",
    "    model.fit(representative_df_train)\n",
    "\n",
    "    #  Obtain the representative test ts\n",
    "    representative_df_test = create_representative_test(test_df)\n",
    "    \n",
    "    #Generate forecasts on the test period for the representative test ts\n",
    "    representative_forecasts = model.predict(df=representative_df_test.drop('y', axis=1))\n",
    "\n",
    "    # Create the adjustment series and combine together train, test and all adjustment series\n",
    "    df_actual_all = create_adjustment_series(train_df, test_df, representative_df_train, representative_df_test, representative_forecasts)\n",
    "\n",
    "    ####  Generate forecasts for each countries time series \n",
    "\n",
    "    #Create adjustment forecasts  by fitting a simple linear model to the adjustment series\n",
    "    adjustment_forecasts_df = df_actual_all.groupby(['Country']).apply(lambda x: get_linear_model_pred(x, representative_df_test)).reset_index(drop=False)\n",
    "    adjustment_forecasts_df = adjustment_forecasts_df.rename(columns={'Year': 'ds'})\n",
    "\n",
    "    #Merge all data so far with the adjustment forecasts\n",
    "    final_output = pd.merge(df_actual_all, adjustment_forecasts_df, on=['Country', 'ds'], how='left')\n",
    "    \n",
    "    #Create forecasts for all individual time series in a cluster by adding together the representative forecasys and adjustment forecasts\n",
    "    final_output['final_prediction'] = final_output['yhat'] + final_output['adjustment_Forecasts_test']\n",
    "\n",
    "    # Store forecasts in a diactionary based on the current cluster\n",
    "    forecasts_clusters[cluster] = final_output\n",
    "\n",
    "    \n",
    "\n",
    "    # Save the current forecast DataFrame as a CSV file\n",
    "    file_name = f'forecasts_cluster{cluster}.csv'\n",
    "    file_path = os.path.join(forecast_folder_path, file_name)\n",
    "    forecasts_clusters[cluster].to_csv(file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of              Country         ds  y_tr  y_repr_tr  adjusted_val  y_tst  \\\n",
       "0        Afghanistan 1950-01-01  27.7       36.4          -8.7    NaN   \n",
       "1             Angola 1950-01-01  36.3       36.4          -0.1    NaN   \n",
       "2              Benin 1950-01-01  35.0       36.4          -1.4    NaN   \n",
       "3             Bhutan 1950-01-01  30.1       36.4          -6.3    NaN   \n",
       "4       Burkina Faso 1950-01-01  33.1       36.4          -3.3    NaN   \n",
       "...              ...        ...   ...        ...           ...    ...   \n",
       "3235            Togo 2021-01-01   NaN        NaN           NaN   61.6   \n",
       "3236          Uganda 2021-01-01   NaN        NaN           NaN   62.7   \n",
       "3237  Western Sahara 2021-01-01   NaN        NaN           NaN   70.8   \n",
       "3238           Yemen 2021-01-01   NaN        NaN           NaN   63.8   \n",
       "3239          Zambia 2021-01-01   NaN        NaN           NaN   61.2   \n",
       "\n",
       "           yhat  level_1  adjustment_Forecasts_test  final_prediction  \n",
       "0           NaN      NaN                        NaN               NaN  \n",
       "1           NaN      NaN                        NaN               NaN  \n",
       "2           NaN      NaN                        NaN               NaN  \n",
       "3           NaN      NaN                        NaN               NaN  \n",
       "4           NaN      NaN                        NaN               NaN  \n",
       "...         ...      ...                        ...               ...  \n",
       "3235  58.041653     16.0                   0.116718         58.158371  \n",
       "3236  58.041653     16.0                 -11.668531         46.373122  \n",
       "3237  58.041653     16.0                  31.495871         89.537524  \n",
       "3238  58.041653     16.0                  21.927893         79.969546  \n",
       "3239  58.041653     16.0                 -29.219208         28.822445  \n",
       "\n",
       "[3240 rows x 10 columns]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
